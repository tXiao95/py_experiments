{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Learning Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review all of the different ways we've optimized the same problem - classification of MNIST data by comparing each objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Least Squares - 2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "E(\\textbf{C}_{\\text{obs}}, \\textbf{W})=\\Vert \\textbf{WY} - \\textbf{C}_{\\text{obs}} \\Vert^2\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Classification Softmax - Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "E(\\textbf{C}_{\\text{obs}}, \\textbf{W})=-\\frac{1}{n}\\textbf{e}^\\top_{n_c}(\\textbf{C}_{\\text{obs}} \\odot \\textbf{WY}) \\textbf{e}_n + \n",
    "\\frac{1}{n}\\log(\\textbf{e}^\\top_{n_c}\\exp(\\textbf{WY}))\\textbf{e}_n\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single-layer feedforward neural network - regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "E(\\textbf{C}_{\\text{obs}}, \\textbf{W}, \\textbf{K})=\\Vert \\textbf{W}\\sigma(\\textbf{KY})-\\textbf{C}_{\\text{obs}}) \\Vert^2\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single-layer feedforward neural network - classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "E(\\textbf{C}_{\\text{obs}}, \\textbf{W}, \\textbf{K})=-\\frac{1}{n}\\textbf{e}^\\top_{n_c}(\\textbf{C}_{\\text{obs}} \\odot \\textbf{W}\\sigma(\\textbf{KY})) \\textbf{e}_n + \n",
    "\\frac{1}{n}\\log(\\textbf{e}^\\top_{n_c}\\exp(\\textbf{W}\\sigma(\\textbf{KY})))\\textbf{e}_n\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when comparing the single layer networks to the linear scenario, the only difference is that the $\\textbf{Y}$ input feature\n",
    "matrix is transformed into $\\sigma(\\textbf{KY})$. We apply a matrix $\\textbf{K}$ to expand the matrix, and then change the rank by applying the nonlinear activation function $\\sigma$. What deep learning essentially does is increase the rank of the matrix, which is usually an undetermined system, to improve the conditions of the optimization problem (make it convex, for example). The idea of Extreme Learning Machines is that if we can randomize the $\\textbf{K}$, then we can just use existing methods for \n",
    "solving the linear regression and classification case without having to resort to previous iterative methods like backpropagation for finding the weights for both $\\textbf{W}$ and $\\textbf{K}$. It removes the optimization for $\\textbf{K}$, and let's us just focus on the $\\textbf{W}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_c = 3\n",
    "n_f = 2\n",
    "n = 100\n",
    "m = 50\n",
    "\n",
    "Y = np.random.normal(size = (n_f, n))\n",
    "C = np.random.uniform(size = (n_c, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELM(Y, C, m):\n",
    "    \"\"\"\n",
    "    Implementation of the Extreme Learning Machine Regression. So easy!\n",
    "    \n",
    "    Parameters\n",
    "        Y -- input feature matrix (n_f x n)\n",
    "        C -- output label matrix (n_c x n)\n",
    "        m -- number of hidden nodes\n",
    "        \n",
    "    Returns\n",
    "        W -- weight matrix (n_c x m)\n",
    "        S -- transformed feature matrix (m x n)\n",
    "    \"\"\"\n",
    "    n_f, n = Y.shape\n",
    "    n_c = C.shape[0]\n",
    "    \n",
    "    # Add bias vector\n",
    "    b = np.ones((1, n))\n",
    "    Y = np.vstack((Y, b))\n",
    "    \n",
    "    # Randomize the weights and biases from input layer to hidden layer\n",
    "    K = np.random.uniform(size = (m, n_f + 1))\n",
    "    # Elementwise activation function to increase rank\n",
    "    S = np.tanh(K @ Y)\n",
    "    # Solve the linear system\n",
    "    W = np.linalg.lstsq(S.T, C.T)[0]\n",
    "    \n",
    "    return W.T, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsiao3/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    }
   ],
   "source": [
    "W, S = ELM(Y, C, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002656694159921141"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(W @ S - C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thsiao3/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "D = np.linalg.lstsq(Y.T, C.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.040964175188037"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(D.T @ Y - C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
