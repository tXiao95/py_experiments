{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with Steepest Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Steepest Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to try out a small example of autodiff by using it to solve the steepest descent least squares problem. We then implement a couple tests to ensure our derivatives are correct. We 1) compare to the closed form solution 2) use a test based on Taylor series introduced by Lars. SDLS works as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "f(x)=\\Vert{Ax-b}\\Vert^2 \\\\\n",
    "\\min_{x}f(x) \\\\\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from vector calculus the steepest descent direction from a point $x$ is the negative gradient evaluated at that point.\n",
    "So we just need to solve for the solution to the equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "\\nabla f(x)=\\frac{\\partial}{\\partial x} f(x)=A^T(Ax-b)=0\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the exact line search problem to optimize our step size $\\alpha$, we solve another optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "x_{(i+1)}=x_{(i)}-\\alpha\\nabla f(x_{(i)}) \\\\\n",
    "\\min_{\\alpha}\\Vert{Ax_{(i+1)}-b}\\Vert^2 \\\\\n",
    "\\frac{\\partial}{\\partial \\alpha} f(x_{(i+1)}) = \\nabla f(x_{(i+1)})^T \\frac{\\partial}{\\partial \\alpha} x_{(i+1)} \n",
    "= -\\nabla f(x_{(i+1)})^T \\nabla f(x_{(i)})\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that the step size that guarantees the lowest error in the direction of the previous gradient, is the one \n",
    "that makes the next gradient orthogonal to the previous gradient. Hence the zig-zagging path we see in SDLS with exact line search. With further decomposition, we can get a closed form solution to $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "\\nabla f(x_{(i+1)})^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(A^T(Ax_{(i+1)}-b))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(A^T(A(x_{(i)}+\\alpha\\nabla f(x_{(i)})-b))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(A^T(Ax_{(i)}+\\alpha A\\nabla f(x_{(i)})-b))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(A^T(Ax_{(i)}-b+\\alpha A\\nabla f(x_{(i)}))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(\\nabla f(x_{(i)})+\\alpha A^T A\\nabla f(x_{(i)}))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(\\nabla f(x_{(i)})^T+\\alpha \\nabla f(x_{(i)})^T A^T A) \\nabla f(x_{(i)})=0 \\\\\n",
    "\\alpha = \\frac{\\nabla f(x_{(i)})^T \\nabla f(x_{(i)})}{\\nabla f(x_{(i)})^T A^T A \\nabla f(x_{(i)})}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use the autodiff on the first optimization problem to compute the gradient of $f(x)$, \n",
    "and use this closed form solution on alpha. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Form Steepest Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "f(x)=\\frac{1}{2}x^TAx-b^Tx+c \\\\\n",
    "\\min_{x}f(x) \\\\\n",
    "\\nabla f(x)=\\frac{\\partial}{\\partial x} f(x)=Ax-b=0 \\\\\\\\\n",
    "x_{(i+1)}=x_{(i)}-\\alpha\\nabla f(x_{(i)}) \\\\\\\\\n",
    "\\min_{\\alpha} \\frac{1}{2} x_{(i+1)}^TAx_{(i+1)} -b^Tx_{(i+1)}+c\\\\\n",
    "\\frac{\\partial}{\\partial \\alpha} f(x_{(i+1)}) = \\nabla f(x_{(i+1)})^T \\frac{\\partial}{\\partial \\alpha} x_{(i+1)} \n",
    "= -\\nabla f(x_{(i+1)})^T \\nabla f(x_{(i)}) \\\\\\\\\n",
    "\\nabla f(x_{(i+1)})^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(A(x_{(i)}+\\alpha\\nabla f(x_{(i)})-b)^T \\nabla f(x_{(i)})=0 \\\\\n",
    "\\alpha = \\frac{\\nabla f(x_{(i)})^T\\nabla f(x_{(i)})}{\\nabla f(x_{(i)})^T A\\nabla f(x_{(i)})}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import autograd as ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, A, b):\n",
    "    \"\"\"\n",
    "    Computes least-squares loss function for a linear system. \n",
    "    \"\"\"\n",
    "    return .5 * np.linalg.norm(A @ x - b)**2\n",
    "\n",
    "def explicit_loss_grad(x, A, b):\n",
    "    \"\"\"\n",
    "    Explicit calculation of gradient of least squares loss function\n",
    "    \"\"\"\n",
    "    return A.T @ (A @ x - b)\n",
    "\n",
    "def line_search(alpha, x, A, b, d):\n",
    "    \"\"\"\n",
    "    Computes least squares loss function wrt to alpha\n",
    "    \"\"\"\n",
    "    return .5 * np.linalg.norm(A @ (x + alpha * d) - b)**2\n",
    "\n",
    "loss_grad = ad.grad(loss)\n",
    "line_search_grad = ad.grad(line_search)\n",
    "\n",
    "def SDLS(A, x, b, maxIter):\n",
    "    n = A.shape[1]\n",
    "    his = np.zeros(maxIter)\n",
    "    grad = np.zeros((n, maxIter))\n",
    "    Xall = np.zeros((n, maxIter))\n",
    "    for i in np.arange(maxIter):\n",
    "        his[i] = np.linalg.norm(A @ x - b)**2\n",
    "        # ************\n",
    "        d = -loss_grad(x, A, b)\n",
    "        #*************\n",
    "        grad[:,i] = d[:,0]\n",
    "        Ad = A @ d\n",
    "        alpha = (d.T @ d) / (Ad.T @ Ad)\n",
    "        x = x + alpha * d\n",
    "        Xall[:,i] = x[:,0]\n",
    "    return x, his, grad, Xall\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 2], [2, 6]])\n",
    "b = np.array([[2],[-8]])\n",
    "x = np.zeros_like(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sol, his, grad, Xall = SDLS(A, x, b, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2152675 ,  1.28907243,  1.21255263,  1.747291  ,  1.72009098,\n",
       "         1.9101711 ,  1.90050248,  1.96806908,  1.96463224,  1.98864971,\n",
       "         1.98742804,  1.99596538,  1.99553112,  1.99856584,  1.99841148,\n",
       "         1.99949021,  1.99943534,  1.99981879,  1.99979928,  1.99993559,\n",
       "         1.99992865,  1.9999771 ,  1.99997464,  1.99999186,  1.99999098,\n",
       "         1.99999711,  1.9999968 ,  1.99999897,  1.99999886,  1.99999963,\n",
       "         1.9999996 ,  1.99999987,  1.99999986,  1.99999995,  1.99999995,\n",
       "         1.99999998,  1.99999998,  1.99999999,  1.99999999,  2.        ,\n",
       "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
       "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
       "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
       "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
       "         2.        ,  2.        ,  2.        ,  2.        ,  2.        ,\n",
       "         2.        ,  2.        ,  2.        ,         nan,         nan],\n",
       "       [-0.94717699, -1.28907243, -1.62575955, -1.747291  , -1.86697107,\n",
       "        -1.9101711 , -1.95271303, -1.96806908, -1.9831912 , -1.98864971,\n",
       "        -1.99402508, -1.99596538, -1.99787613, -1.99856584, -1.99924504,\n",
       "        -1.99949021, -1.99973164, -1.99981879, -1.99990461, -1.99993559,\n",
       "        -1.99996609, -1.9999771 , -1.99998795, -1.99999186, -1.99999572,\n",
       "        -1.99999711, -1.99999848, -1.99999897, -1.99999946, -1.99999963,\n",
       "        -1.99999981, -1.99999987, -1.99999993, -1.99999995, -1.99999998,\n",
       "        -1.99999998, -1.99999999, -1.99999999, -2.        , -2.        ,\n",
       "        -2.        , -2.        , -2.        , -2.        , -2.        ,\n",
       "        -2.        , -2.        , -2.        , -2.        , -2.        ,\n",
       "        -2.        , -2.        , -2.        , -2.        , -2.        ,\n",
       "        -2.        , -2.        , -2.        , -2.        , -2.        ,\n",
       "        -2.        , -2.        , -2.        , -2.        , -2.        ,\n",
       "        -2.        , -2.        , -2.        ,         nan,         nan]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.80000000e+01, 2.41715373e+01, 8.59210613e+00, 3.05418256e+00,\n",
       "       1.08565129e+00, 3.85909716e-01, 1.37176928e-01, 4.87614299e-02,\n",
       "       1.73329224e-02, 6.16122618e-03, 2.19009277e-03, 7.78498665e-04,\n",
       "       2.76728081e-04, 9.83668110e-05, 3.49658389e-05, 1.24290894e-05,\n",
       "       4.41809115e-06, 1.57047140e-06, 5.58245707e-07, 1.98436131e-07,\n",
       "       7.05368582e-08, 2.50732985e-08, 8.91264958e-09, 3.16812415e-09,\n",
       "       1.12615340e-09, 4.00306748e-10, 1.42294552e-10, 5.05805598e-11,\n",
       "       1.79795573e-11, 6.39108147e-12, 2.27179802e-12, 8.07541923e-13,\n",
       "       2.87051910e-13, 1.02036558e-13, 3.62703027e-14, 1.28927797e-14,\n",
       "       4.58291689e-15, 1.62906110e-15, 5.79072511e-16, 2.05839361e-16,\n",
       "       7.31685645e-17, 2.60088240e-17, 9.24515442e-18, 3.28630891e-18,\n",
       "       1.16813547e-18, 4.15220896e-19, 1.47595592e-19, 5.24638974e-20,\n",
       "       1.86509335e-20, 6.63032453e-21, 2.35745779e-21, 8.38204444e-22,\n",
       "       2.97759314e-22, 1.05794745e-22, 3.75512598e-23, 1.33424849e-23,\n",
       "       4.70327806e-24, 1.65506351e-24, 5.85461009e-25, 2.08164616e-25,\n",
       "       7.19417480e-26, 2.42921827e-26, 1.07450744e-26, 4.69628618e-27,\n",
       "       1.82384641e-27, 7.32062920e-28, 1.82424084e-28, 6.31088724e-29,\n",
       "       0.00000000e+00,            nan])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.00000000e+01,  9.84766335e+00, -3.55463784e+00,\n",
       "         3.50048768e+00, -1.26354502e+00,  1.24429660e+00,\n",
       "        -4.49144494e-01,  4.42302377e-01, -1.59654601e-01,\n",
       "         1.57222477e-01, -5.67514288e-02,  5.58868965e-02,\n",
       "        -2.01730776e-02,  1.98657677e-02, -7.17079851e-03,\n",
       "         7.06156097e-03, -2.54895917e-03,  2.51012918e-03,\n",
       "        -9.06062674e-04,  8.92260019e-04, -3.22072467e-04,\n",
       "         3.17166123e-04, -1.14485098e-04,  1.12741070e-04,\n",
       "        -4.06953061e-05,  4.00753674e-05, -1.44657075e-05,\n",
       "         1.42453418e-05, -5.14203513e-06,  5.06370311e-06,\n",
       "        -1.82780726e-06,  1.79996308e-06, -6.49719293e-07,\n",
       "         6.39821692e-07, -2.30951673e-07,  2.27433452e-07,\n",
       "        -8.20949571e-08,  8.08443663e-08, -2.91817814e-08,\n",
       "         2.87372584e-08, -1.03730615e-08,  1.02150688e-08,\n",
       "        -3.68726427e-09,  3.63106789e-09, -1.31069999e-09,\n",
       "         1.29068489e-09, -4.65897543e-10,  4.58790783e-10,\n",
       "        -1.65595981e-10,  1.63101532e-10, -5.88631366e-11,\n",
       "         5.79878368e-11, -2.09365858e-11,  2.06004103e-11,\n",
       "        -7.43849426e-12,  7.31059657e-12, -2.65698574e-12,\n",
       "         2.57616151e-12, -9.30810984e-13,  9.11271059e-13,\n",
       "        -3.38395978e-13,  3.18856053e-13, -9.14823772e-14,\n",
       "         1.38999923e-13, -4.44089210e-14,  5.32907052e-14,\n",
       "        -1.99840144e-14,  1.42108547e-14,             nan,\n",
       "                    nan],\n",
       "       [-4.40000000e+01, -2.23810531e+00, -1.56404065e+01,\n",
       "        -7.95565382e-01, -5.55959808e+00, -2.82794681e-01,\n",
       "        -1.97623577e+00, -1.00523268e-01, -7.02480246e-01,\n",
       "        -3.57323811e-02, -2.49706287e-01, -1.27015674e-02,\n",
       "        -8.87615415e-02, -4.51494721e-03, -3.15515134e-02,\n",
       "        -1.60490022e-03, -1.12154204e-02, -5.70483906e-04,\n",
       "        -3.98667576e-03, -2.02786368e-04, -1.41711885e-03,\n",
       "        -7.20832097e-05, -5.03734430e-04, -2.56229705e-05,\n",
       "        -1.79059347e-04, -9.10803805e-06, -6.36491130e-05,\n",
       "        -3.23757767e-06, -2.26249546e-05, -1.15084161e-06,\n",
       "        -8.04235200e-06, -4.09082516e-07, -2.85876489e-06,\n",
       "        -1.45414015e-07, -1.01618740e-06, -5.16894154e-08,\n",
       "        -3.61217846e-07, -1.83737123e-08, -1.28399888e-07,\n",
       "        -6.53118537e-09, -4.56415403e-08, -2.32161312e-09,\n",
       "        -1.62239155e-08, -8.25259860e-10, -5.76696557e-09,\n",
       "        -2.93340463e-10, -2.04991935e-09, -1.04262377e-10,\n",
       "        -7.28682004e-10, -3.70601327e-11, -2.59055000e-10,\n",
       "        -1.31850086e-11, -9.20836740e-11, -4.68602934e-12,\n",
       "        -3.27045058e-11, -1.67510450e-12, -1.15987220e-11,\n",
       "        -5.87085935e-13, -4.08562073e-12, -2.13162821e-13,\n",
       "        -1.44417811e-12, -5.68434189e-14, -5.16919840e-13,\n",
       "        -2.75335310e-14, -2.20268248e-13, -1.42108547e-14,\n",
       "        -7.54951657e-14, -7.10542736e-15,             nan,\n",
       "                    nan]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does it return nan at 100 iterations? NaN's start appearing in the loss when the loss hits zero, but one before that in the X's and the gradient. Can see this by seeing how back propagation/reverse accumulation works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "y = D(C(x)) \\\\ \n",
    "\\frac{\\partial y}{\\partial x} = \\frac{\\partial d}{\\partial c} \\frac{\\partial c}{\\partial x} \\\\ \n",
    "\\end{gather}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
