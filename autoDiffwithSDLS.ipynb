{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with Steepest Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to try out a small example of autodiff by using it to solve the steepest descent least squares problem. We then implement a couple tests to ensure our derivatives are correct. We 1) compare to the closed form solution 2) use a test based on Taylor series introduced by Lars. SDLS works as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "f(x)=\\Vert{Ax-b}\\Vert^2 \\\\\n",
    "\\min_{x}f(x) \\\\\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from vector calculus the steepest descent direction from a point $x$ is the negative gradient evaluated at that point.\n",
    "So we just need to solve for the solution to the equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "\\frac{\\partial}{\\partial x} f(x)=\\nabla f(x)=A^T(Ax-b)=0\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the exact line search problem to optimize our step size $\\alpha$, we solve another optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "x_{(i+1)}=x_{(i)}-\\alpha\\nabla f(x_{(i)}) \\\\\n",
    "\\min_{\\alpha}\\Vert{Ax_{(i+1)}-b}\\Vert^2 \\\\\n",
    "\\frac{\\partial}{\\partial \\alpha} f(x_{(i+1)}) = \\nabla f(x_{(i+1)})^T \\frac{\\partial}{\\partial \\alpha} x_{(i+1)} \n",
    "= -\\nabla f(x_{(i+1)})^T \\nabla f(x_{(i)})\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that the step size that guarantees the lowest error in the direction of the previous gradient, is the one \n",
    "that makes the next gradient orthogonal to the previous gradient. Hence the zig-zagging path we see in SDLS with exact line search. With further decomposition, we can get a closed form solution to $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "\\nabla f(x_{(i+1)})^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(A^T(Ax_{(i+1)}-b))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(A^T(A(x_{(i)}+\\alpha\\nabla f(x_{(i)})-b))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(A^T(Ax_{(i)}+\\alpha A\\nabla f(x_{(i)})-b))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(A^T(Ax_{(i)}-b+\\alpha A\\nabla f(x_{(i)}))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(\\nabla f(x_{(i)})+\\alpha A^T A\\nabla f(x_{(i)}))^T \\nabla f(x_{(i)})=0 \\\\\n",
    "(\\nabla f(x_{(i)})^T+\\alpha \\nabla f(x_{(i)})^T A^T A) \\nabla f(x_{(i)})=0 \\\\\n",
    "\\alpha = \\frac{\\nabla f(x_{(i)})^T \\nabla f(x_{(i)})}{\\nabla f(x_{(i)})^T A^T A \\nabla f(x_{(i)})}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use the autodiff on the first optimization problem to compute the gradient of $f(x)$, \n",
    "and use this closed form solution on alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import autograd as ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, A, b):\n",
    "    \"\"\"\n",
    "    Computes least-squares loss function for a linear system. \n",
    "    \"\"\"\n",
    "    return .5 * np.linalg.norm(A @ x - b)**2\n",
    "\n",
    "def line_search(alpha, x, A, b, d):\n",
    "    \"\"\"\n",
    "    Computes least squares loss function wrt to alpha\n",
    "    \"\"\"\n",
    "    return .5 * np.linalg.norm(A @ (x + alpha * d) - b)**2\n",
    "\n",
    "loss_grad = ad.grad(loss_function)\n",
    "line_search_grad = ad.grad(line_search)\n",
    "\n",
    "def SDLS(A, x, b, maxIter):\n",
    "    n = A.shape[1]\n",
    "    his = np.zeros(maxIter)\n",
    "    Xall = np.zeros((n, maxIter))\n",
    "    for i in np.arange(maxIter):\n",
    "        # AUTODIFF HERE\n",
    "        d = -loss_grad(x, A, b)\n",
    "        # AUTODIFF HERE\n",
    "        Ad = A @ d\n",
    "        alpha = (d.T @ d) / (Ad.T @ Ad)\n",
    "        x = x + alpha * d\n",
    "    return x\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 2], [2, 6]])\n",
    "b = np.array([[2],[-8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros_like(b)\n",
    "#x = np.array([2,-2])[np.newaxis].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.]\n",
      " [-2.]]\n"
     ]
    }
   ],
   "source": [
    "solution = SDLS(A, x, b, 50)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "solution = SDLS(A, x, b, 100)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does it return nan at 100 iterations? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
