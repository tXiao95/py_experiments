{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation for Conjugate Gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In steepest descent algorithms with exact line search, the basic intuition is that we choose a starting point, say $x_{(0)}$. \n",
    "From this starting point, we see which direction in the vector space $x$ resides in guarantees the most dramatic decrease\n",
    "in the value of our objective function. To know how far to go in that direction before the objective function no longer decreases, we see what step size guarantees to give the\n",
    "lowest value of our objective function, in that direction $d_{(i)}$. \n",
    "\n",
    "While this sounds like a strategy that would work well in a lot of cases, there are scenarios where steepest descent struggles. It's also not the most efficient. With exact line search, one sees that the direction the iterations take are orthogonal to the step right before it (the optimization of step size $\\alpha$ guarantees that the gradients at each successive iteration must be orthogonal). This results in a series of zig-zag pattern. But what if you could just collapse all iterations that travel in the same direction into one step? For example in the 2D case, if I was able to do that, My algorithm would stop after only 2 steps (only 2 orthogonal directions in $\\mathbb{R}^2$ but could be many more in steepest descent especially if my problem is ill-conditioned.\n",
    "\n",
    "## The Method of Conjugate Directions\n",
    "\n",
    "Orthogonality is one of numerical analysts best friends, and in this case it comes into play yet again. We know that if our solution is in a vector space of dimension $n$, say $\\mathbb{R}^n$, then our algorithm would travel at most $n$ steps. If our starting point happens to be along one of the orthogonal directions, it could arrive at the solution in even less time! But therein lies our first problem. How do we choose what directions $d_0, d_1, d_2, \\dots d_n$ to travel in, given that we start at $x_0$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd as ag\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x, y):\n",
    "    \"\"\"Compute the softmax of vector x.\"\"\"\n",
    "    exps = np.exp(np.array([x, y]))\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "der = ag.grad(ag.grad(ag.grad(ag.grad(softmax))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid()\n",
    "x = np.array([a,a])\n",
    "y1 = softmax(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
